"""
    Вопросы:
    - В чем недостатки этого способа построения спеллчекера?
    - Быстро ли работает ваш спеллчекер? Можно ли его ускорить и если да, то как?
    - Приведите интересные примеры правильных слов, которые этот спеллчекер считает ошибочными и наоборот,
    слов с опечаткой, которые считаются им правильными.

    О программе:
    Ниже представлено 2 спеллчекера, которые используют разные способы проверки
    У двух спеллчекеров есть один существенный недостаток-они не учитывают контекст.
    Пример:
        Он перешла дорогу на красный

    Спеллчекеры не смогут найти ошибку, так как они обращают внимание только орфографические ошибки, а грамматические
    пропускают (не видят)
    Также существует проблема, когда имена собственные совпадают с ошибочными словами:
    Пример:
        Привет, миня зовут Петя
    Вывод:
        Привет миня зовут Петя
    Все из-за того, что в мире есть река Миня в Северо-Байкальском районе.
    Убрать имена собственные нельзя, так как в этом случае спеллчекер не будет ловить ошибки.
    Пример, если убрать имена собственные:
        Кем был Вошингтон?
    Вывод:
        Кем был Вошингтон

    Ниже описаны плюсы и минусы каждого алгоритма
"""


import csv
import re
from spellchecker import SpellChecker
import re
from collections import Counter
from symspellpy import SymSpell, Verbosity
import pandas as pd


def _csv_writer(data, path):    # Записывает данные в csv файл (нужно использовать только 1 раз для загрузки файла)
    frame = pd.DataFrame(data)
    frame.to_csv(path, index=False, sep=' ')
"""
    Ниже прдеставлена стратегия промаха (Near-Miss Strategy). Подход Питера Норвига.
    Ифнормация об алгоритме:
        Если слово не найдено в словаре, то генерируются все возможные комбинации слова с заданным расстоянием 
        редактирования (операции удаления, перестановки, замены и вставки) и выполняется их поиск в словаре. 
        Если эти шаги приводят к слову, содержащемуся в словаре, то оценивается расстояние до исходного слова. 
        Чтобы измерить близость слов, используется расстояние Левенштейна
    Минусы:
        1) Долгое время работы (особенно на длинных словах)

    Способ улучшения:
    Отличный способ улучшения описал Филипп @bak на хабре
    Ссылка на статью: https://habr.com/en/post/346618/
"""


def pyspellchecker(sentence):
    misspelled = spell.unknown(re.split(r'\W+', sentence))
    correct_word_list = []
    wrong_word_list = []
    for word in misspelled:
        wrong_word_list.append(word)
        correct_word_list.append(spell.correction(word))
    return f'\nВаше предложение: {sentence}\n' \
           f'Опечатка в словах: {wrong_word_list}\n' \
           f'Наиболее вероятная правильная форма: {correct_word_list}'

'''
    Второй спеллчекер использует орфографическую коррекцию на основе симметричного удаления.
    Пример работы:
        темт
    Для слова темт «удаления» будут следующими: емт, тмт, тет, тем
    Поскольку удаление тет содержится в индексе, то это означает, что слову с опечаткой темт соответствует слово тест.
    
    Также в алгоритме учтено то, что разные слова в словаре могут приводить к одной и той же модификации.
    В этом случае список кандидатов сортируется сначала по расстоянию Левенштейна до исходного слова, 
    затем по частоте встречаемости.
    Минусы:
        1) Высокое потребление памяти
    Плюсы:
        1) Алгоритм быстрый ( О(1) )
        2) Исправляет ошибки, где слова слипаются
        
    Способ улучшения:
    В статье на хабре описан способ, который может уменьшить потребеление памяти.
    
    "
        Итоговая адаптация алгоритма SymSpell получилась следующей:
        Будем хранить все удаления слов из оригинального словаря в bloom-фильтре. 
        При поиске кандидатов будем вначале делать удаления от исходного слова на нужную глубину (аналогично SymSpell). 
        Но, в отличии от SymSpell, следующим шагом для каждого удаления будем делать вставки, 
        и проверять получившееся слово в оригинальном словаре. 
        А индекс с удалениями, хранящийся в bloom-фильтре, будем использовать чтобы пропускать вставки для тех удалений, 
        которые в нём отсутствуют. 
        В этом случае ложные срабатывания нам не страшны — мы просто выполним немного лишней работы.
    
        Производительность получившегося решения практически не замедлилась, 
        а используемая память сократилась очень существенно — до 140 Мб (примерно в 25 раз). 
        В итоге общий размер памяти сократился с 7 Гб до 400 Мб.
    "
    
    Также добалю, что данный алгортим требует, чтобы словарь хранился в текстовом формате в папке самой библиотеки, что
    вызывает неудобства. Проблема может решиться, если использовать другую библиотеку
'''


def symspell(sentence):

    suggestions = sym_spell.lookup_compound(sentence,
                                            max_edit_distance=2,
                                            transfer_casing=True)
    return f'\n\nВаш предложение: {sentence}\n' \
           f'Наиболее вероятная правильная форма: {suggestions[0]}'


"""
    Из-за размера файла Python выдает ошибку "memoryerror". Она связана, наверное, с тем, что я использую 32-битную 
    версию.
    Проблема решается, если данные разделить на равные части и обработать.
"""


def words(text):
    return re.findall(r'\w+', text[:round(len(text)/10)].lower()) + \
           re.findall(r'\w+', text[round(len(text)/10):round(len(text)*2/10)].lower()) +\
           re.findall(r'\w+', text[round(len(text)*2/10):round(len(text)*3/10)].lower()) + \
           re.findall(r'\w+', text[round(len(text)*3/10):round(len(text)*4/10)].lower()) + \
           re.findall(r'\w+', text[round(len(text)*4/10):round(len(text)*5/10)].lower()) + \
           re.findall(r'\w+', text[round(len(text)*5/10):round(len(text)*6/10)].lower()) + \
           re.findall(r'\w+', text[round(len(text)*6/10):round(len(text)*7/10)].lower()) + \
           re.findall(r'\w+', text[round(len(text)*7/10):round(len(text)*8/10)].lower()) + \
           re.findall(r'\w+', text[round(len(text)*8/10):round(len(text)*9/10)].lower())


if __name__ == "__main__":

    WORDS = Counter(words(open('Standart_text.txt',
                               newline='',
                               encoding='utf-8').read()))
    print("Идет подготовка данных...")

    spell = SpellChecker()
    spell._word_frequency.load_text_file('data.csv')

    array = WORDS.items()   # Нужно использовать только 1 раз
    _csv_writer(WORDS.items(),
                'C:\\Users\\79859\\AppData\\Roaming\\Python\\Python38\\site-packages\\symspellpy\\Symtext.txt') # Нужно использовать только 1 раз

    sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)
    sym_spell.load_dictionary(
        'C:\\Users\\79859\\AppData\\Roaming\\Python\\Python38\\site-packages\\symspellpy\\Symtext.txt',
        term_index=0,
        count_index=1,
        encoding='utf-8')

    sentence = input('Введите предложение:')

    print(pyspellchecker(sentence))

    print(symspell(sentence))